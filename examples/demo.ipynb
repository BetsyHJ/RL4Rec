{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "py3",
   "display_name": "Python 3.6.9 64-bit ('py3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Play with SOFA: the Simulator for OFfline leArning and evaluation.\n",
    "## Load SOFA first"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set conf and load sofa\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import numpy as np\n",
    "from time import time, localtime, strftime\n",
    "import configparser, random\n",
    "\n",
    "from env.env import SOFAEnv, simulated_data\n",
    "\n",
    "def _get_conf(conf_name):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"../conf/\"+conf_name+\".properties\")\n",
    "    conf=dict(config.items(\"default\"))\n",
    "    if ('seed' in conf) and (conf['seed'].lower() != 'none'):\n",
    "        seed = int(conf['seed'])\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    # # for multiple jobs in \n",
    "    # args = set_hparams()\n",
    "    # conf[\"data.debiasing\"] = args.debiasing\n",
    "    # conf[\"seed\"] = str(args.seed)\n",
    "\n",
    "    evalProcess = conf['evaluation']\n",
    "    if evalProcess.lower() == 'false':\n",
    "        if (conf[\"data.input.dataset\"] in ['sim4', 'sim5']) and (conf[\"data.debiasing\"] == 'GT'):\n",
    "            rating_file = conf[\"data.input.path\"] + conf[\"data.input.dataset\"] + \"_GT_ratingM.ascii\"\n",
    "        else:\n",
    "            rating_file = conf[\"data.input.path\"] + conf[\"data.input.dataset\"] + '_' + \\\n",
    "            conf[\"data.gen_model\"] + '_' + conf[\"data.debiasing\"] + \"_ratingM.ascii\"\n",
    "            if conf[\"data.debiasing\"] == 'GT':\n",
    "                rating_file = conf[\"data.input.path\"] + conf[\"data.input.dataset\"] + \"_pseudoGT_ratingM.ascii\"\n",
    "                print(\"we use a pseudo GT for yahoo, which is generated by MF on unbiased testset:\", rating_file)\n",
    "    else:\n",
    "        if conf[\"data.input.dataset\"].lower() in ['sim4', 'sim5']:\n",
    "            print('now evaluation process only for simulated dataset which has the groundTruth')\n",
    "            rating_file = conf[\"data.input.path\"] + conf[\"data.input.dataset\"] + \"_GT_ratingM.ascii\"\n",
    "        elif conf[\"data.input.dataset\"].lower() in [\"yahoo\", \"coat\"]:\n",
    "            rating_file = conf[\"data.input.path\"] + conf[\"data.input.dataset\"] + '_' + \\\n",
    "            conf[\"data.gen_model\"] + '_' + conf[\"data.debiasing\"] + \"_ratingM.ascii\" # this simulator is not for evaluation directly, but for several interaction to generate states\n",
    "            # solution-2 with pseudo GT\n",
    "            rating_file = conf[\"data.input.path\"] + conf[\"data.input.dataset\"] + \"_pseudoGT_ratingM.ascii\"\n",
    "            print(\"we use a pseudo GT for yahoo, which is generated by MF on unbiased testset:\", rating_file)\n",
    "        else:\n",
    "            print(\"check data\")\n",
    "    conf[\"RATING_TYPE\"] = conf[\"rating_type\"]\n",
    "    conf[\"RATINGS\"] = np.clip(np.round(np.loadtxt(rating_file)).astype('int'), 1, 5)\n",
    "    conf[\"EPISODE_LENGTH\"] = conf[\"episode_length\"]\n",
    "    conf['mode'] = conf['mode'].upper()\n",
    "    if conf['mode'] == 'DOUBLEDQN':\n",
    "        conf['mode'] = 'DoubleDQN'\n",
    "    return conf\n",
    "\n",
    "conf = _get_conf('yahoo')\n",
    "sofa = SOFAEnv(conf)"
   ]
  },
  {
   "source": [
    "## Details in SOFA\n",
    "The number of users and items."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of users and items are 300 and 300\n"
     ]
    }
   ],
   "source": [
    "action_space = sofa.num_items\n",
    "num_users = sofa.num_users\n",
    "print(\"The number of users and items are %d and %d\" % (num_users, action_space))"
   ]
  },
  {
   "source": [
    "## Basic operation of SOFA\n",
    "### Reset \n",
    "To load a new user, and set an empty state\n",
    "### Step: \n",
    "Receive an action from rec-policy, and return the state, reward and done (a signal if user leaves the system.)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Round-0, recommend item 0 to user 1, and we can observe reward 0\nRound-1, recommend item 83 to user 1, and we can observe reward 0\nRound-2, recommend item 235 to user 1, and we can observe reward 0\nRound-3, recommend item 282 to user 1, and we can observe reward 0\nRound-4, recommend item 74 to user 1, and we can observe reward 1\nRound-5, recommend item 250 to user 1, and we can observe reward 0\nRound-6, recommend item 56 to user 1, and we can observe reward 0\nRound-7, recommend item 131 to user 1, and we can observe reward 0\nRound-8, recommend item 79 to user 1, and we can observe reward 0\nRound-9, recommend item 156 to user 1, and we can observe reward 1\nThe last state is [[0, 83, 235, 282, 74, 250, 56, 131, 79, 156], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# a user enter the system\n",
    "user = 1\n",
    "sofa.reset(user)\n",
    "# we do 10-turn recommendation\n",
    "for i in range(10):\n",
    "    action = np.random.randint(action_space)\n",
    "    state, reward, done = sofa.step(action)\n",
    "    print(\"Round-%d, recommend item %d to user %d, and we can observe reward %d\" % \\\n",
    "        (i, action, user, reward))\n",
    "print(\"The last state is\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Round-0, recommend item 74 to user 1, and we can observe reward 1\nRound-1, recommend item 74 to user 1, and we can observe reward 0\nRound-2, recommend item 74 to user 1, and we can observe reward 0\nRound-3, recommend item 74 to user 1, and we can observe reward 0\nRound-4, recommend item 74 to user 1, and we can observe reward 0\nRound-5, recommend item 74 to user 1, and we can observe reward 0\nRound-6, recommend item 74 to user 1, and we can observe reward 0\nRound-7, recommend item 74 to user 1, and we can observe reward 0\nRound-8, recommend item 74 to user 1, and we can observe reward 0\nRound-9, recommend item 74 to user 1, and we can observe reward 0\nThe last state is [[74, 74, 74, 74, 74, 74, 74, 74, 74, 74], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# # repeat recommendation would not receive click from simulated user twice\n",
    "# a user enter the system\n",
    "user = 1\n",
    "sofa.reset(user)\n",
    "# we do 10-turn recommendation\n",
    "action = 74\n",
    "for i in range(10):\n",
    "    state, reward, done = sofa.step(action)\n",
    "    print(\"Round-%d, recommend item %d to user %d, and we can observe reward %d\" % \\\n",
    "        (i, action, user, reward))\n",
    "print(\"The last state is\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}